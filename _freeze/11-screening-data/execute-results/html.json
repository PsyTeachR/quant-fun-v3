{
  "hash": "a3dc361a6de209129d3dc21a96c0c090",
  "result": {
    "engine": "knitr",
    "markdown": "# Missing data, outliers, and checking assumptions\n\n\n\n\n\nIn this chapter...\n\n**Chapter Intended Learning Outcomes (ILOs)**\n\nBy the end of this chapter, you will be able to: \n\n- ILO1. \n\n## Chapter preparation\n\n### Organising your files and project for the chapter\n\nFor this chapter, we are going to revisit the data sets you worked with in Chapters 8 [@dawtry_why_2015] and 9 [@lopez_visual_2023]. They each presented some useful examples for checking statistical assumptions and the decisions that go into data analysis. We might not use both data sets for each topic we cover, but they will be useful to demonstrate some of the problems and decisions we highlighted in previous chapters, but did not explore solutions.\n\nBefore we can get started, you need to organise your files and project for the chapter, so your working directory is in order.\n\n1. In your folder for research methods and the book `ResearchMethods1_2/Quant_Fundamentals`, create a new folder called `Chapter_11_screening_data`. Within `CChapter_11_screening_data`, create two new folders called `data` and `figures`.\n\n2. Create an R Project for `Chapter_11_screening_data` as an existing directory for your chapter folder. This should now be your working directory.\n\n3. Create a new R Markdown document and give it a sensible title describing the chapter, such as `11 Missing Data, Outliers, and Assumptions`. Delete everything below line 10 so you have a blank file to work with and save the file in your `Chapter_11_screening_data` folder. \n\n4. The @dawtry_why_2015 data wrangling steps were quite long, so please save this clean version of the data to focus on screening data in this chapter: [Dawtry_2015_clean.csv](data/Dawtry_2015_clean.csv). You will also need to save the data from @lopez_visual_2023 if you have not downloaded it yet: [Lopez_2023.csv](data/Lopez_2023.csv). Right click the link and select \"save link as\", or clicking the link will save the files to your Downloads. Make sure that you save the files as \".csv\". Save or copy the files to your `data/` folder within `Chapter_11_screening_data`.\n\nYou are now ready to start working on the chapter! \n\n### Activity 1 - Read and wrangle the data\n\nAs the first activity, try and test yourself by completing the following task list to read and wrangle the two data files. There is nothing extra to do with this version of the Dawtry data and one small step for the Lopez data. \n\n::: {.callout-tip}\n#### Try this\n\nTo read and wrangle the data, complete the following tasks: \n\n1. Load the following packages:\n\n    - <pkg>performance</pkg>\n    \n    - <pkg>tidyverse</pkg>\n\n2. Read the data file `data/Dawtry_2015_clean.csv` to the object name `dawtry_clean`.\n\n2. Read the data file `data/Lopez_2023.csv` to the object name `lopez_data`.\n\n3. Create a new object called `lopez_clean` based on `lopez_data`:\n\n    - Create a new variable called `Condition_label` by recoding `Condition`. \"0\" is the \"Control\" group and \"1\" is the \"Experimental\" group. \n\n:::\n\n::: {.callout-caution collapse=\"true\"}\n#### Show me the solution\nYou should have the following in a code chunk: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# load the relevant packages\nlibrary(performance)\nlibrary(tidyverse)\n\n# Read the Dawtry_2015_clean.csv file \ndawtry_clean <- read_csv(\"data/Dawtry_2015_clean.csv\")\n\n# Read the Lopez_2023.csv file \nlopez_data <- read_csv(\"data/Lopez_2023.csv\")\n\n# recode condition\nlopez_clean <- lopez_data %>% \n  mutate(Condition_label = case_match(Condition,\n                                      0 ~ \"Control\",\n                                      1 ~ \"Experimental\"))\n```\n:::\n\n\n:::\n\n## Missing data\n\nChecking whether data are missing are relatively straight forward. Missing values in a spreadsheet will be recorded as NA and there are a few ways of identifying them. The much more difficult part of missing data is considering *why* they are missing in the first place. For example, it might be because: \n\n- Your participants accidentally missed a question.\n\n- You made a mistake while setting up your questionnaire/experiment and some responses did not save.\n\n- Your participants intentionally did not want to answer a question.\n\n- Your participants did not turn up to a final testing session.\n\nFor the first two reasons, it is not ideal as we are losing data but there is no systematic pattern to why the data is missing. For the latter two reasons, there might be a relationship between a key variable and whether the data are missing. This is where it is particularly important to consider the role of missing data. We are focusing on data skills here rather than the conceptual understanding, but missing data are commonly categorised as: \n\n- Missing completely at random.\n\n- Missing at random. \n\n- Missing not at random. \n\nFor this introductory course, we do not have time to investigate strategies to address missing data apart from focusing on complete cases and ignoring missing data, but you might find @jakobsen_when_2017 useful if you want to explore options like data imputation. \n\n### Identifying missing data\n\nReturning to data skills, the simplest way of getting an overview of whether any data are missing is using the `summary()` function. For this part, we will focus on @dawtry_why_2015.  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(dawtry_clean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       PS      Household_Income Political_Preference      age      \n Min.   :  1   Min.   :    20   Min.   :1.000        Min.   :19.0  \n 1st Qu.: 77   1st Qu.: 25000   1st Qu.:3.000        1st Qu.:28.0  \n Median :153   Median : 42000   Median :4.000        Median :33.5  \n Mean   :153   Mean   : 54732   Mean   :4.465        Mean   :37.4  \n 3rd Qu.:229   3rd Qu.: 75000   3rd Qu.:6.000        3rd Qu.:46.0  \n Max.   :305   Max.   :350000   Max.   :9.000        Max.   :69.0  \n               NA's   :4        NA's   :4            NA's   :1     \n     gender     Population_Inequality_Gini_Index Population_Mean_Income\n Min.   :1.00   Min.   :14.26                    Min.   : 14205        \n 1st Qu.:1.00   1st Qu.:31.10                    1st Qu.: 47250        \n Median :1.00   Median :35.66                    Median : 58650        \n Mean   :1.48   Mean   :35.51                    Mean   : 58605        \n 3rd Qu.:2.00   3rd Qu.:40.73                    3rd Qu.: 67875        \n Max.   :2.00   Max.   :57.45                    Max.   :138645        \n NA's   :3                                                             \n Social_Circle_Inequality_Gini_Index Social_Circle_Mean_Income\n Min.   : 2.00                       Min.   : 12000           \n 1st Qu.:19.79                       1st Qu.: 36000           \n Median :25.59                       Median : 51060           \n Mean   :26.35                       Mean   : 54294           \n 3rd Qu.:33.27                       3rd Qu.: 66375           \n Max.   :61.36                       Max.   :148500           \n                                                              \n fairness_satisfaction redistribution\n Min.   :1.000         Min.   :1.00  \n 1st Qu.:2.000         1st Qu.:3.25  \n Median :3.000         Median :4.00  \n Mean   :3.539         Mean   :3.91  \n 3rd Qu.:5.000         3rd Qu.:4.75  \n Max.   :9.000         Max.   :6.00  \n                                     \n```\n\n\n:::\n:::\n\n\nWe get a range of summary statistics for each variable but importantly for our purposes here, the final entry is `NA's` where relevant. We can see there are 4 missing values for household income, 4 for political preference, 1 for age, and 3 for gender. \n\n::: {.callout-tip}\n#### Try this\nIf you explore `lopez_clean` from @lopez_visual_2023, do we have any missing data to worry about? <select class='webex-select'><option value='blank'></option><option value='answer'>Yes</option><option value=''>No</option></select> \n:::\n\n::: {.callout-caution collapse=\"true\"} \n#### Solution\n\nYes, it looks like there is also a small amount of missing data here. There is 1 for sex, 2 for estimated ounces, and 3 for estimates calories. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(lopez_clean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n ParticipantID       Sex              Age          Ethnicity    \n Min.   :1001   Min.   :0.0000   Min.   :18.00   Min.   :1.000  \n 1st Qu.:1202   1st Qu.:1.0000   1st Qu.:19.00   1st Qu.:3.000  \n Median :1462   Median :1.0000   Median :20.00   Median :3.000  \n Mean   :1456   Mean   :0.8099   Mean   :20.47   Mean   :3.261  \n 3rd Qu.:1704   3rd Qu.:1.0000   3rd Qu.:21.00   3rd Qu.:4.000  \n Max.   :1928   Max.   :3.0000   Max.   :54.00   Max.   :8.000  \n                NA's   :1                                       \n   OzEstimate       CalEstimate      M_postsoup     F_CaloriesConsumed\n Min.   :  0.010   Min.   :  1.0   Min.   : 0.600   Min.   :  13.31   \n 1st Qu.:  2.000   1st Qu.: 50.0   1st Qu.: 5.575   1st Qu.: 123.65   \n Median :  4.000   Median : 90.0   Median : 8.700   Median : 192.97   \n Mean   :  6.252   Mean   :124.6   Mean   :10.203   Mean   : 226.30   \n 3rd Qu.:  8.000   3rd Qu.:160.0   3rd Qu.:13.125   3rd Qu.: 291.11   \n Max.   :100.000   Max.   :800.0   Max.   :46.200   Max.   :1024.72   \n NA's   :2         NA's   :3                                          \n   Condition      Condition_label   \n Min.   :0.0000   Length:464        \n 1st Qu.:0.0000   Class :character  \n Median :0.0000   Mode  :character  \n Mean   :0.4698                     \n 3rd Qu.:1.0000                     \n Max.   :1.0000                     \n                                    \n```\n\n\n:::\n:::\n\n\n:::\n\n### Removing missing data\n\nOnce we know whether missing data are present, we must consider what to do with them. For this chapter, we are only going to control removing participants, but you could apply a data imputation technique at this point. \n\nFor all the modelling techniques we apply in this book, the functions will remove participants who have one or more missing values from any variable involved in the analysis. The functions will give you a warning to highlight when this happens, but it is normally a good idea to remove participants with missing data yourself so you have a note of how many participants you remove. \n\nFor `dawtry_clean`, the <pkg>tidyverse</pkg> function `drop_na()` is the easiest way of removing missing data, either participants with any missing data or by specifying individual variables. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndawtry_all_missing <- dawtry_clean %>% \n  drop_na()\n\ndawtry_income_missing <- dawtry_clean %>% \n  drop_na(Household_Income)\n```\n:::\n\n\nWe can compare the number of participants by using the `nrow()` function to count how many rows are in each object. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# How many rows in the full data? \nnrow(dawtry_clean)\n\n# How many rows when we remove missing data in one variable? \nnrow(dawtry_income_missing)\n\n# How many rows when we remove any missing value?\nnrow(dawtry_all_missing)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 305\n[1] 301\n[1] 294\n```\n\n\n:::\n:::\n\n\nLike most data skills and statistics concepts, the key skill here comes in decision making; documenting and justifying the approach that you take. \n\n## Outliers\n\nThe next data screening concept revolves around identifying potential outliers. Like missing data, the difficulty here comes in first deciding what an outlier is and then deciding on what to do with it. @leys_how_2019 mention one study found 14 definitions and 39 unique ways of identifying outliers, so this is our second key area of decision making. Leys et al. categorise outliers into three types: \n\n1. Error outliers - a mistake or impossible value. \n\n2. Interesting outliers - values that looks extreme until you take a moderator into account. \n\n3. Random outliers - values that are extreme compared to the majority of data points. \n\nEven simpler, we can consider values as legitimate or not legitimate. Error outliers would be not legitimate as they represent a mistake or error, so they would potentially provide misleading results. These are values you can justify removing or correcting as they should not be there in the first place. \n\nInteresting and random outliers would be legitimate as they are not clear mistakes or errors; they are just different to the majority of values in the data. In most cases, it is not a good idea to remove these kind of values as they potentially tell you something interesting, but you might need to approach the data analysis in a different way to ensure the results are robust to extreme values. \n\n### Identifying error outliers\n\nUnless you can specifically identify values or participants you know contain errors, the main way to check is by ensuring the values are within known limits. \n\nWe can look at `dawtry_clean` and the key variables we explored in Chapter 8. Fairness and satisfaction was on a 1-9 scale, so we can check the minimum and maximum values and create a plot. For example, we can isolate the variable and apply the `summary()` function. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndawtry_clean %>% \n  select(fairness_satisfaction) %>% \n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n fairness_satisfaction\n Min.   :1.000        \n 1st Qu.:2.000        \n Median :3.000        \n Mean   :3.539        \n 3rd Qu.:5.000        \n Max.   :9.000        \n```\n\n\n:::\n:::\n\n\nThe minimum and maximum values are nice and consistent with what we expect. \n\nFor a visual check, we can also plot the minimum and maximum possible values on a boxplot. This is just a check for you, so you do not need to worry so much about the presentation.  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndawtry_clean %>% \n  ggplot(aes(y = fairness_satisfaction, x = \"\")) + # make x blank \n  geom_boxplot() + \n  scale_y_continuous(limits = c(1, 9), \n                     breaks = seq(1, 9, 1)) + \n  geom_hline(yintercept = c(1, 9), # min and max values\n             linetype = 2) # create dashed line\n```\n\n::: {.cell-output-display}\n![](11-screening-data_files/figure-html/unnamed-chunk-7-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n::: {.callout-tip}\n#### Try this\nIf you explore `redistribution` from `dawtry_clean`, the minimum and maximum values are 1-6. Does it look like there are any problematic looking values? <select class='webex-select'><option value='blank'></option><option value=''>Yes</option><option value='answer'>No</option></select>\n:::\n\n::: {.callout-caution collapse=\"true\"} \n#### Solution\n\nNo, it looks like all values are within the expected 1-6 range. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndawtry_clean %>% \n  select(redistribution) %>% \n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n redistribution\n Min.   :1.00  \n 1st Qu.:3.25  \n Median :4.00  \n Mean   :3.91  \n 3rd Qu.:4.75  \n Max.   :6.00  \n```\n\n\n:::\n:::\n\n\nWe can also confirm this with a visual check. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndawtry_clean %>% \n  ggplot(aes(y = redistribution, x = \"\")) + # make x blank \n  geom_boxplot() + \n  scale_y_continuous(limits = c(1, 6), \n                     breaks = seq(1, 6, 1)) + \n  geom_hline(yintercept = c(1, 6), # min and max values\n             linetype = 2) # create dashed line\n```\n\n::: {.cell-output-display}\n![](11-screening-data_files/figure-html/unnamed-chunk-9-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n\nIf you did identify error outliers to remove, then you could use `filter()` (Chapter 5) to directly remove values outside your known range, or you could first use `case_when()` to code observations as outliers or not (Chapter 4), before deciding to filter them out. \n\n### Identifying interesting or random outliers\n\nIdentifying error outliers relies on manually setting known minimum and maximum values, whereas identifying interesting or random outliers relies on data driven boundaries. For this example, we focus on univariate outliers, where we focus on one variable at a time. When we return to checking assumptions of regression models, you can identify interesting or random outliers through observations with large leverage / Cook's distance values. \n\nIn general, we recommend not removing outliers providing you are confident they are not errors. It is better to focus on modelling your outcome in a more robust way. However, it is also important you know how to identify errors for strategies you will come across in published research. \n\nWe focus here on setting boundaries using the median absolute deviation as recommended by @leys_how_2019. You will see other approaches in the literature, but this method is useful as it's influenced less by the very outliers it is trying to identify. We will use `lopez_clean` from @lopez_visual_2023 for this section. \n\nThere are two main steps to this process because we have two groups and each group will have different boundaries. If you only have individual variables, then you could just `mutate()` your data, without the initial `group_by()` and `summarise()` step.\n\nFirst, we group by the condition to get one value per group. We then calculate a few values for the median ounces of soup, 3 times the MAD in line with Leys et al., then calculating the upper and lower bound using these objects. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# create a new object with values per group\nmad_bounds <- lopez_clean %>% \n  group_by(Condition_label) %>% \n  summarise(oz_median = median(M_postsoup), # median of soup in oz\n            oz_MAD = 3 * mad(M_postsoup), # 3 times the MAD\n            lower = oz_median - oz_MAD, # lower bound \n            upper = oz_median + oz_MAD) # upper bound\n\nmad_bounds\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|Condition_label | oz_median|   oz_MAD|    lower|    upper|\n|:---------------|---------:|--------:|--------:|--------:|\n|Control         |       7.8| 14.67774| -6.87774| 22.47774|\n|Experimental    |      10.6| 17.56881| -6.96881| 28.16881|\n\n</div>\n:::\n:::\n\n\nIn this example, the lower bound is lower than 0 as the smallest possible value. The upper bounds are then between 22 and 28 depending on the group. \n\nSecond, we must add these values to the other information we have available. We join the data sets using `Condition_label`. This adds the relevant values to each group. We then use `mutate()` and `case_when()` to label values as outliers or not. If they are outside the lower and upper bounds, they are labelled as \"outliers\". If they are inside the lower and upper bounds, they are labelled as \"no outliers\". \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlopez_mad <- lopez_clean %>% \n  inner_join(mad_bounds, by = \"Condition_label\") %>% \n  mutate(oz_outlier = case_when(M_postsoup < lower | M_postsoup > upper ~ \"Outlier\",\n                                M_postsoup >= lower | M_postsoup <= upper ~ \"No Outlier\"))\n```\n:::\n\n\nWe can use these in one of two ways. First, we can visualise the presence of outliers by adding coloured points. These are checks for you again, so you do not need to worry about the plot formatting. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlopez_mad %>% \n  ggplot(aes(x = Condition_label, y = M_postsoup)) + \n  geom_boxplot() + \n  geom_point(aes(colour = oz_outlier)) # needs to be within aes to set dynamic values\n```\n\n::: {.cell-output-display}\n![](11-screening-data_files/figure-html/unnamed-chunk-12-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\nWe can see a few values per group flagged as outliers using this criterion. If you did decide to remove outliers, then you could use filter to remove them: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlopez_remove <- lopez_mad %>% \n  filter(oz_outlier == \"No Outlier\")\n```\n:::\n\n\n::: {.callout-tip}\n#### Try this\nIf you switch to `dawtry_clean` from @dawtry_why_2015, apply the MAD procedure to the variable `fairness_satisfaction`. Does it look like there are any outliers using this criterion? <select class='webex-select'><option value='blank'></option><option value=''>Yes</option><option value='answer'>No</option></select>.\n:::\n\n::: {.callout-caution collapse=\"true\"} \n#### Solution\n\nNo, none of the values are outside the MAD thresholds. The thresholds are well beyond the minimum and maximum possible values of 1-9 for this variable.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndawtry_mad <- dawtry_clean %>% \n  mutate(fs_median = median(fairness_satisfaction), # median of fairness/satisfaction\n         fs_MAD = 3 * mad(fairness_satisfaction), # 3 times the MAD\n         lower = fs_median - fs_MAD, # lower bound \n         upper = fs_median + fs_MAD,  # upper bound\n         fs_outlier = case_when(fairness_satisfaction < lower | fairness_satisfaction > upper ~ \"Outlier\",\n                                fairness_satisfaction >= lower | fairness_satisfaction <= upper ~ \"No Outlier\"))\n```\n:::\n\n\nFor this variable and it's bounded scale, no value is above or below the thresholds. You can see this in the data, or add horizontal lines in a plot since we are only plotting one variable. The dashed lines are the MAD thresholds and the solid lines are the minimum and maximum possible values.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndawtry_mad %>% \n  ggplot(aes(y = fairness_satisfaction, x = \"\")) + \n  geom_boxplot() + \n  geom_hline(aes(yintercept = lower), \n             linetype = 2) + # dashed line\n  geom_hline(yintercept = c(1, 9)) + \n  geom_hline(aes(yintercept = upper), \n             linetype = 2) + \n  scale_y_continuous(limits = c(-4, 10), \n                     breaks = seq(-4, 10, 2))\n```\n\n::: {.cell-output-display}\n![](11-screening-data_files/figure-html/unnamed-chunk-15-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n:::\n\nRemember: identifying outliers is a crucial researcher degree of freedom, so pre-register your choice of outlier detection wherever possible, and document how many outliers you removed. We still recommend favouring a more robust model, but you can make an informed decision now you know how to identify outliers in the data. \n\n## Checking assumptions\n\nThe final section revisits checking assumptions from Chapter 8 and 9. In those chapters, we introduced the concepts and stuck with the output regardless of whether we were happy with the assumptions or not. In this chapter, we will introduce potential solutions. \n\nAs a reminder, the assumptions for simple linear regression are:\n\n1. The outcome is interval/ratio level data.\n\n2. The predictor variable is interval/ratio or categorical (with two levels at a time).\n\n3. All values of the outcome variable are independent (i.e., each score should come from a different participant/observation).\n\n4. The predictors have non-zero variance.\n\n5. The relationship between the outcome and predictor is linear.\n\n6. The residuals should be normally distributed.\n\n7. There should be homoscedasticity.\n\nAssumptions 1-4 are pretty straight forward as they relate to your understanding of the design or a simple check on the data. On the other hand, assumptions 5-7 require diagnostic checks. \n\nFor this part, we focus on @lopez_visual_2023 as the assumptions did not look quite right in Chapter 9. As a reminder, we can get a quick diagnostic check by running `plot()` on the model object: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Condition as a factor containing 0 and 1\nlm_cals_numbers <- lm(formula = F_CaloriesConsumed ~ Condition, \n                      data = lopez_clean)\n\n# Change the panel layout to 2 x 2\npar(mfrow = c(2,2))\n\n# plot the diagnostic plots \nplot(lm_cals_numbers)\n```\n\n::: {.cell-output-display}\n![](11-screening-data_files/figure-html/unnamed-chunk-16-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\nAll of the checks look good apart from normality. We have a clear deviation from the line to curve around at lower and higher values along the x-axis. In Chapter 9, we said we would stick with it as it was consistent with the original article's analyses, but now we will outline options. \n\n### Do nothing, parametric tests are robust \n\nOne get out jail free card is doing nothing as the parametric tests are robust to violations of assumptions. You will often see this brought up and it is largely true under certain conditions. @knief_violating_2021 report a simulation study where they explore how violating assumptions like linearity, normality, and outliers affect the results of parametric statistical tests like simple linear regression. They found the tests were robust to violations - particularly normality - apart from when there were extreme outliers which could bias the results. \n\nIn the Lopez et al. example, we identified a few outliers using the 3 times the MAD criterion and the strictest Cook's distance cut-off in Chapter 9, but normality was the only notable problem in the diagnostic checks. One option would be to feel reassured the results are robust to minor violations and explain that in your report. \n\nThe second option would be checking the results are robust to the presence of outliers. Remember, we advise excluding outliers as a last resort if you consider them legitimate, but it provides a robustness check to see if you get similar conclusions with and without the outliers. For example, we can exclude outliers using the MAD criterion and check the results:  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# remove outliers under 3 * MAD \n\nlm_cals_outliers <- lm(formula = F_CaloriesConsumed ~ Condition, \n                      data = filter(lopez_mad, \n                                    oz_outlier == \"No Outlier\"))\n\nsummary(lm_cals_outliers)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = F_CaloriesConsumed ~ Condition, data = filter(lopez_mad, \n    oz_outlier == \"No Outlier\"))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-220.65  -87.56  -15.30   65.84  369.35 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  182.573      7.485  24.393  < 2e-16 ***\nCondition     66.906     10.903   6.136 1.85e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 115.7 on 450 degrees of freedom\nMultiple R-squared:  0.07722,\tAdjusted R-squared:  0.07517 \nF-statistic: 37.66 on 1 and 450 DF,  p-value: 1.852e-09\n```\n\n\n:::\n:::\n\n\nThe conclusions are very similar. The difference is still statistically significant and instead of a 63 calorie difference for the slope, we get a 67 calorie difference. So, we can stick with the original results and feel reassured that the results are robust to the presence of outliers, given the criterion we used. You would explain to the reader you checked the robustness of the results and what your final conclusion was.  \n\n### Treat the data as non-parametric\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Condition_label as numbers\nlm_cals_ranks <- lm(formula = rank(F_CaloriesConsumed) ~ 1 + Condition, \n                     data = lopez_clean)\n\nsummary(lm_cals_ranks)\n\nwilcox.test(formula = F_CaloriesConsumed ~ Condition_label, \n             data = lopez_clean, \n            conf.int = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = rank(F_CaloriesConsumed) ~ 1 + Condition, data = lopez_clean)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-261.138 -110.361    1.862  111.467  263.967 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   200.03       8.27  24.188  < 2e-16 ***\nCondition      69.11      12.06   5.728 1.84e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 129.7 on 462 degrees of freedom\nMultiple R-squared:  0.0663,\tAdjusted R-squared:  0.06428 \nF-statistic: 32.81 on 1 and 462 DF,  p-value: 1.836e-08\n\n\n\tWilcoxon rank sum test with continuity correction\n\ndata:  F_CaloriesConsumed by Condition_label\nW = 18827, p-value = 3.021e-08\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -82.06643 -39.92418\nsample estimates:\ndifference in location \n             -59.88637 \n```\n\n\n:::\n:::\n\n\n### Use an alternative model \n\n## Test yourself\n\nTo end the chapter, we have some knowledge check questions to test your understanding of the concepts we covered in the chapter. We then have some error mode tasks to see if you can find the solution to some common errors in the concepts we covered in this chapter. \n\n### Knowledge check\n\n**Question 1**.\n\n### Error mode\n\nThe following questions are designed to introduce you to making and fixing errors. For this topic, we focus on the new types of data visualisation. Remember to keep a note of what kind of error messages you receive and how you fixed them, so you have a bank of solutions when you tackle errors independently. \n\nCreate and save a new R Markdown file for these activities. Delete the example code, so your file is blank from line 10.\n\n...\n\n## Words from this Chapter\n\nBelow you will find a list of words that were used in this chapter that might be new to you in case it helps to have somewhere to refer back to what they mean. The links in this table take you to the entry for the words in the [PsyTeachR Glossary](https://psyteachr.github.io/glossary/){target=\"_blank\"}. Note that the Glossary is written by numerous members of the team and as such may use slightly different terminology from that shown in the chapter.\n\n\n::: {.cell layout-align=\"center\"}\n<div class=\"kable-table\">\n\n\n</div>\n:::\n\n\n## End of Chapter\n\nThat is the final chapter you will complete for Research Methods 1. \n\n",
    "supporting": [
      "11-screening-data_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}