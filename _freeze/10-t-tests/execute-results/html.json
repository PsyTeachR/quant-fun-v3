{
  "hash": "79993d70a311dd8780035548807df9b2",
  "result": {
    "markdown": "# t-tests\n\nExperiments where you compare results from two conditions or two groups are very common within Psychology as often we want to know if there is an effect of a given variable.  One of the really confusing things however about research design is that there are many names for the same type of design. To clarify:\n\n* <a class='glossary'>One-sample<span class='def'></span></a> are used to study one group of people against a known norm or criterion - for example, comparing the mean IQ of a sample against a known population norm such as an IQ of 100.\n* Independent-samples and <a class='glossary'>between-subjects<span class='def'></span></a> designs mean the same thing - different participants in different conditions.\n* In contrast, <a class='glossary'>within-subjects<span class='def'></span></a>, dependent-samples, paired-samples, and repeated-measures all tend to mean the same participants in all conditions\n* Matched-pairs design means different people in different conditions but you have matched participants across the conditions so that they are effectively the same person (e.g. age, IQ, Social Economic Status, etc)\n* <a class='glossary'>Mixed-design<span class='def'></span></a> is when there is a combination of within-subjects and between-subjects designs in the one experiment. For example, say you are looking at attractiveness and dominance of male and female faces. Everyone might see both male and female faces (within) but half of the participants do ratings of attractiveness and half of the participants do ratings of trustworthiness (between).\n\nTo get a better understanding of how some of these tests run we will look at running an example of a between-subjects t-test and a within-subjects t-test through a series of activities. Remember that the solutions are at the bottom of the page if you are stuck, and please do ask questions on the forums.\n\n## Between-Subjects t-tests (two-sample)\n\nWe will begin by looking at the between-subjects t-test which is used for comparing the outcome in two groups of different people. Here we will be using data from Schroeder and Epley (2015) on the perception of people from their job applications. You can take a look at the Psychological Science article here, [Schroeder, J. and Epley, N. (2015). The sound of intellect: Speech reveals a thoughtful mind, increasing a job candidate's appeal. Psychological Science, 26, 277--891.](https://doi.org/10.1177/0956797615572906){target=\"_blank\"}, if you like but it is not essential for completing the activities. The abstract from this article explains more about the different experiments conducted, and we will be specifically looking at the data set from Experiment 4, based on information from the [Open Stats Lab](https://sites.trinity.edu/osl/data-sets-and-activities/t-test-activities){target=\"_blank\"}. The abstract reads:\n\n> A person's mental capacities, such as intellect, cannot be observed directly and so are instead inferred from indirect cues. We predicted that a person's intellect would be conveyed most strongly through a cue closely tied to actual thinking: his or her voice. Hypothetical employers (Experiments 1-3b) and professional recruiters (Experiment 4) watched, listened to, or read job candidates' pitches about why they should be hired. These evaluators (the employers) rated a candidate as more competent, thoughtful, and intelligent when they heard a pitch rather than read it and, as a result, had a more favourable impression of the candidate and were more interested in hiring the candidate. Adding voice to written pitches, by having trained actors (Experiment 3a) or untrained adults (Experiment 3b) read them, produced the same results. Adding visual cues to audio pitches did not alter evaluations of the candidates. For conveying one's intellect, it is important that one's voice, quite literally, be heard.\n\nTo summarise, 39 professional recruiters from Fortune 500 companies evaluated job pitches of M.B.A. candidates from the University of Chicago Booth School of Business. The methods and results appear on pages 887-889 of the article if you want to look at them specifically for more details and the original data, in **wide** format, can be found at the [Open Stats Lab](https://drive.google.com/open?id=0Bz-rhZ21ShvOei1MM24xNndnQ00){target=\"_blank\"} website for later self-directed learning. Today however, we will be working with a modified version in \"tidy\" format which can be downloaded below and what we plan to do is reproduce the results from the article on Pg 887.\n\n### Data and Descriptives\n\nAs always, the first activity is about getting ourselves ready to analyse the data so try out the steps and if you need help, consult the earlier chapters.\n\n#### Activity 1: Set-up {#ttest-a1}\n\n* Open RStudio and set the working directory to your chapter folder. Ensure the environment is clear.\n    * If you're using the Rserver, avoid a number of issues by restarting the session - click `Session` - `Restart R`\n* Open a new R Markdown document and save it in your working directory. Call the file \"ttests\".   \n* Download <a href=\"evaluators.csv\" download>evaluators.csv</a> and <a href=\"ratings.csv\" download>ratings.csv</a> and save them in your t-test folder. Make sure that you do not change the file names at all.\n  * If you prefer you can download the data in a [zip folder by clicking here](data/chpt10/PsyTeachR_FQA_Chpt10-data-between.zip){target=\"_blank\"}\n  * Remember not to change the file names at all and that `data.csv` is not the same as `data (1).csv`.\n* Delete the default R Markdown welcome text and insert a new code chunk that loads the following packages, in this specific order, using the `library()` function. Remember the solutions if needed.\n  * Load the packages in this order, `Hmisc`, `broom`, `car`,`effectsize`, `report`, and `tidyverse`\n  * again we have not used some of these packages so you will likely need to install some of them using `install.packages()`. Remember though that you should only do this on your own machine and only in the console window. If you are using the RServer you will not need to install them.\n* Finally, load the data held in `evaluators.csv` as a tibble into an object named `evaluators` using `read_csv()`. \n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\nRemember to have a look at your data to help you understand the structure and the layout of the data. You can do this in whatever way you prefer. \n\nNow that we have our data, and have explored it, there is a few things we can do to make working with it a bit easier. If you look at the data, and in particular the `sex` column, you will see it is actually coded as <a class='glossary'>numeric<span class='def'></span></a> but we will want to treat it as <a class='glossary'>categorical<span class='def'></span></a>. Secondly, it can be tricky to work with 1s and 2s when you mean people, so we can \"recode\" the variables into labels that are easier to work with. That is what we will do here using a combination of `mutate()`, which we already know, and the `recode()` function from the `dplyr` package that is loaded in as part of the `tidyverse`, and the `as.factor()` function from `base`. Converting categorical data to factors will make it easier to work with in visualisations and analysis.\n\n#### Activity 2: Explore the dataset {#ttest-a2}\n\nIn a new code chunk, copy the code below and see if you can follow it.\n\n* First we use `mutate()` and `recode()` to recode `sex` into a new variable called `sex_labels` so that `1` = `male` and `2` = `female`. \n  * Be careful using `recode()` as there are multiple functions in different packages called with the same name so it is better to use the `package::function()` approach and specify `dplyr::recode()` to get the right one.\n* Then we use `mutate()` and `as.factor()` to overwrite `sex_labels` and `condition` as factors.  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nevaluators <- evaluators %>%\n  mutate(sex_labels = dplyr::recode(sex, \"1\" = \"male\", \"2\" = \"female\"),\n         sex_labels = as.factor(sex_labels),\n         condition = as.factor(condition))\n```\n:::\n\n\nNow see if you can create a count of the different sex labels to answer the following question. One approach would be `group_by() %>% count()` but what would you group by? Maybe store this tibble in an object called `eval_counts`.\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n* How many participants were noted as being female: <input class='webex-solveme nospaces' size='2' data-answer='[\"30\"]'/>\n* How many participants were noted as being male: <input class='webex-solveme nospaces' size='1' data-answer='[\"4\"]'/>\n* How many data points are missing for `sex`? <input class='webex-solveme nospaces' size='1' data-answer='[\"5\"]'/>\n\n#### Activity 3: Ratings {#ttest-a3}\n\nExcellent work. Our evaluator data is ready to work with and we are now going to calculate what is called an \"overall intellect rating\" given by each evaluator, calculated by averaging the ratings of `competent`, `thoughtful` and `intelligent` from each evaluator; held within `ratings.csv`. This overall rating will measure how intellectual the evaluators thought candidates were, depending on whether or not the evaluators **read** or **listened** to the candidates' resume pitches. **Note**, however, we are not looking at ratings to individual candidates; we are looking at overall ratings for each evaluator. This is a bit confusing but makes sense if you stop to think about it a little. What we are interested in is how the medium they received the resume impacted their rating of the candidate. Once we have done that, we will then combine the overall intellect rating with the overall impression ratings and overall hire ratings for each evaluator, with the end goal of having a tibble called `ratings2` - which has the following structure:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n| eval_id|Category   | Rating|condition |sex_labels |\n|-------:|:----------|------:|:---------|:----------|\n|       1|hire       |  6.000|listened  |female     |\n|       1|impression |  7.000|listened  |female     |\n|       1|intellect  |  6.000|listened  |female     |\n|       2|hire       |  4.000|listened  |female     |\n|       2|impression |  4.667|listened  |female     |\n|       2|intellect  |  5.667|listened  |female     |\n:::\n:::\n\n\nThe following steps describe how to create the above tibble and it would be good practice to try this out yourself. Look at the table and think what do I need? The trick when doing data analysis and data wrangling is to first think about what you want to achieve - the end goal - and then think about what functions you need to use to get there. The solution is hidden just below the stpes of course if you want to look at it. Let's look at the steps. Steps 1, 2 and 3 calculate the new overall `intellect` rating. Steps 4 and 5 combine this rating to all other information.\n\n1. Load the data found in `ratings.csv` as a tibble into an object called `ratings`. (e.g. read the csv)\n\n2. `filter()` only the relevant variables (**thoughtful**, **competent**, **intelligent**) into a new tibble stored in an objected called something useful (we will call ours `iratings`), and then calculate a mean `Rating` for each evaluator (e.g. group_by & summarise).  \n\n3. Add on a new column called `Category` where every entry is the word `intellect`. This tells us that every number in this tibble is an intellect rating.  (e.g. mutate)\n\n4. Now create a new tibble called `ratings2` and filter into it just the \"impression\" and \"hire\" ratings from the original `ratings` tibble. \n\n5. Next, bind this tibble with the tibble you created in step 3 to bring together the intellect, impression, and hire ratings, in `ratings2`.  (e.g. `bind_rows(object1, object2)`)\n\n6. Join `ratings2` with the `evaluator` tibble that we created in Task 1 (e.g. `inner_join()`). Keep only the necessary columns as shown above (e.g. `select()`) and arrange by Evaluator and Category (e.g. `arrange()`).\n\n\n<div class='webex-solution'><button>Our approach to this</button>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 1. load in the data\nratings <- read_csv(\"book/ratings.csv\")\n\n# 2. first step: pull out the ratings associated with intellect\niratings <- ratings %>%\n  filter(Category %in% c(\"competent\", \"thoughtful\", \"intelligent\"))\n\n# second step: calculate means for each evaluator\nimeans <- iratings %>%\n  group_by(eval_id) %>%\n  summarise(Rating = mean(Rating))\n\n# 3. add Category variable \n# this way we can combine with 'impression' and 'hire' into a single table, very useful!\nimeans2 <- imeans %>%\n  mutate(Category = \"intellect\")\n\n# 4., 5. & 6. combine into a single table\nratings2 <- ratings %>%\n  filter(Category %in% c(\"impression\", \"hire\")) %>%\n  bind_rows(imeans2) %>%\n  inner_join(evaluators, \"eval_id\") %>%\n  select(-age, -sex) %>%\n  arrange(eval_id, Category)\n```\n:::\n\n\n</div>\n\n\n* Finally, calculate the n, mean and SD for each condition and category to help with reporting the descriptive statistics.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngroup_means <- ratings2 %>%\n  group_by(condition, Category) %>%\n  summarise(n = n(), m = mean(Rating), sd = sd(Rating))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`summarise()` has grouped output by 'condition'. You can override using the\n`.groups` argument.\n```\n:::\n:::\n\n\n### Visualising two groups\n\nBrilliant! Now that we have our data in a workable fashion, we are going to start looking at some visualisations and making figures. You should **always** visualise your data before you run a statistical analysis. Visualisations serve as part of the descriptive measures and they help you interpret the results of the test but they also give you an understanding of the spread of your data as part of the test assumptions. For data with a categorical IV, we are going to look at using the violin-boxplots that we saw in the introduction to visualisation chapter. In the past people would have tended to use barplots but as <a href=\"https://link.springer.com/article/10.3758/s13423-012-0247-5\" target = \"_blank\">Newman and Scholl (2012)</a> point out, barplots are misleading to viewers about how the underlying data actually looks. You can read that paper if you like, for more info, but hopefully by the end of this section you will see why violin-boxplots are more informative.\n\n#### Activity 4: Visualisation {#ttest-a4}\n\nWe will visualise the intellect ratings for the listened and the read conditions. The code we will use to create our figure is as follows with the explanation below. Put this code in a new code chunk and run it.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nratings2 %>%\n  filter(Category == \"intellect\") %>%\nggplot(aes(x = condition, y = Rating)) +\n  geom_violin(trim = TRUE) +\n  geom_boxplot(aes(fill = condition), width = .2, show.legend = FALSE) + \n  stat_summary(geom = \"pointrange\", fun.data = \"mean_cl_normal\")  +\n  labs(x = \"Condition\", y = \"Rating Score\") +\n  geom_jitter(height = .1, width = .2)\n```\n:::\n\n\nThe first part of the code uses a pipe to filter the data to just the intellect rating:\n\n* `ratings %>% filter(Category == \"intellect)` is the same as `filter(ratings, Category == \"intellect\")`\n* this code also reflects nicely the difference between pipes (`%>%`) used in wrangling and the `+` used in the visualisations with ggplot. Notice that we switch from pipes to plus when we start adding layers to our visualisation.\n\nThe main parts of the code to create the violin-boxplot above are:\n\n* ggplot() which creates our base layer and sets our data and our x and y axes.\n* `geom_violin()` which creates the density plot. The reason it is called a violin plot is because if your data are normally distributed it should look something like a violin.  \n* `geom_boxplot()` which creates the boxplot, showing the median and inter-quartile range (see [here](https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51){target=\"_blank\"} if you would like more information). The boxplot can also give you a good idea if the data are skewed - the median line should be in the middle of the box. The more the median is moved towards one of th extremities of the box, the more your data is likely to be skewed.  \n* `geom_jitter()` can be used to show individual data points in your dataset and you can change the width and height of the jitter. Note that this uses a randomised method to display the points so you will get a different output each time you run it.\n* And finally, we will use `stat_summary()` for displaying the mean and confidence intervals. Within this function, `fun.data` specifies the a summary function that gives us the summary of the data we want to plot, in this case, `mean_cl_normal` which will calculate the mean plus the upper and lower confidence interval limits. You could also specify `mean_se` here if you wanted standard error. Finally, `geom` specifies what shape or plot we want to use to display the summary, in this case we want a `pointrange` (literally a point (the mean) with a range (the CI)).\n\nThe figure will look like this:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Violin-boxplot of the evaluator data](10-t-tests_files/figure-html/plot1a-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\nAn alternative version would be this shown below. Perhaps compare the two codes and see if you can see what makes the differences:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nratings2 %>%\n  filter(Category == \"intellect\") %>%\nggplot(aes(x = condition, y = Rating)) +\n  geom_violin(trim = FALSE) +\n  geom_boxplot(aes(fill = condition), width = .2, show.legend = FALSE) + \n  stat_summary(geom = \"pointrange\", fun.data = \"mean_cl_normal\") +\n  labs(x = \"Condition\", y = \"Rating Score\")\n```\n\n::: {.cell-output-display}\n![Violin-boxplot of the evaluator data](10-t-tests_files/figure-html/plot2-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\nTry to answer the following question:\n\n* In which condition did the evaluators give the higher ratings overall? <select class='webex-select'><option value='blank'></option><option value='answer'>listened</option><option value=''>read</option></select>\n* Would the descriptives (means, sds, figure) be inline with the hypothesis that evaluators favour resumes they have listened to more than resumes they have read? <select class='webex-select'><option value='blank'></option><option value='answer'>yes</option><option value=''>no</option></select>\n\nNice and informative figure huh? It gives a good representation of the data in the two conditions, clearly showing the spread and the centre points. If you compare this to Figure 7 in the original paper you see the difference. We actually get much more information with our approach. We even get a sense that maybe the data is questionable on whether it is skewed or not, but more on that below. \n\nThe code is really useful as well so you know it is here if you want to use it again. But maybe have a play with the code to try out things to see what happens. For instance:\n\n* Try setting `trim = TRUE`, `show.legend = FALSE`, and/or altering the value of `width` to see what these arguments do.\n* change the `Category == \"intellect\"` to `Category == \"hire\"` or `Category == \"impression\"` to create visualisations of the other conditions.\n\n### Assumptions\n\nGreat. We have visualised our data as well and we have been able to make some descriptive analysis about what is going on. Now we want to get ready to run the actual analysis. But one final thing we are going to decide is which t-test? But hang on you say, didn't we decide that? We are going to run a between-subjects t-test! Right? Yes! But, and you know what we are about to say, there is more than one between-subjects t-test you can run.  The two common ones are:\n\n* Student's between-subjects t-test\n* Welch's between-subjects t-test\n\nWe are going to recommend that, at least when doing the analysis by code, you should use Welch's between-subjects t-test for the reasons explained in this paper by [Delarce et al,m (2017)](https://www.rips-irsp.com/article/10.5334/irsp.82/){target=\"_blank\"} Now you don't have to read that paper but effectively, the Welch's between-subjects t-test is better at maintaining the false positive rate of your test ($\\alpha$, usually set at $\\alpha$ = .05) at the requested level. So we will show you how to run a Welch's t-test here.\n\nThe assumptions for a Welch's between-subjects t-test are:\n\n1. The data are continuous, i.e. interval/ratio\n2. The data are independent\n3. The residuals are normally distributed for each group\n\nWe know that 1 and 2 are true from the design of the experiment, the measures used, and by looking at the data. To test assumption 3, we can create a Q-Q plots of the **residuals**. For a between-subject t-test the residuals are the difference between the mean of each group and each data point. E.g., if the mean of group A is 10 and a participant in group A scores 12, the residual for that participant is 2.\n\n* Thinking back to your lectures, if you ran a Student's t-test instead of a Welch t-test, what would the 4th assumption be? <select class='webex-select'><option value='blank'></option><option value='answer'>Homogeneity of variance</option><option value=''>Homoscedascity</option><option value=''>Nominal data</option></select>\n\n#### Activity 5: Assumptions {#ttest-a5}\n\n* Run the below code to calculate then plot the residuals for the \"listened\" condition on \"intellect\" ratings. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlistened_intellect_residuals <- ratings2 %>%\n  filter(condition == \"listened\", Category == \"intellect\") %>%\n  mutate(group_resid = Rating - mean(Rating)) %>%\n  select(group_resid)\n\nqqPlot(listened_intellect_residuals$group_resid)\n```\n:::\n\n\n* Run the below code to calculate then plot the residuals for the \"read\" condition on \"intellect\" ratings. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nread_intellect_residuals <- ratings2 %>%\n  filter(condition == \"read\", Category == \"intellect\") %>%\n  mutate(group_resid = Rating - mean(Rating)) %>%\n  select(group_resid)\n\nqqPlot(read_intellect_residuals$group_resid)\n```\n:::\n\n\nIf we then look at our plots we get something that looks like this for the listened condition:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Residual plots of listened condition. Each circle represents an indivudal rater. If data is normally distributed then it should fall close to or on the diagonal line.](10-t-tests_files/figure-html/qqplot3-1.png){fig-align='center' width=100%}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 6 8\n```\n:::\n:::\n\n\nAnd something like this for the read condition.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Residual plots of read intellect condition. Each circle represents an indivudal rater. If data is normally distributed then it should fall close to or on the diagonal line.](10-t-tests_files/figure-html/qqplot4-1.png){fig-align='center' width=100%}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 11 18\n```\n:::\n:::\n\n\nWhat you are looking for is for the data to fall close to the diagonal line. Looking at the plots, maybe we could suggest that the \"listened\" condition is not so great as there is some data points moving away from the line at the far ends. The \"read\" condition seems a bit better, at least subjectively! There will always be some deviation from the diagonal line but at perhaps most of the data in both plots is relatively close to their respective diagonal lines.  \n\nBut in addition to the Q-Q plots we can also run a test on the residuals known as the **Shapiro-Wilk** test. The Shapiro-Wilk's test has the alternative hypothesis that the data is significantly different from normal. As such, if you find a significant result using the test then the interpretation is that your data is not normal. If you find a non-significant finding then the interpretation is that your data is not significantly different from normal. One technical point is that the test doesn't actually say your data is normal either but just that it is not significantly different from normal. Again, remember that assumptions have a degree of subjectivity to them. We use the `shapiro.wilk()` function from the base package to run the Shapiro-Wilk's test.\n\n* In a new code chunk, run both lines of code below and look at their output. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nshapiro.test(x = listened_intellect_residuals$group_resid)\nshapiro.test(x = read_intellect_residuals$group_resid)\n```\n:::\n\n\nTry to answer the following questions:\n\n* According to the Shapiro-Wilk's test, is the data normally distributed for the listened condition? <select class='webex-select'><option value='blank'></option><option value='answer'>Yes</option><option value=''>No</option></select>\n* According to the Shapiro-Wilk's test, is the data normally distributed for the read condition? <select class='webex-select'><option value='blank'></option><option value='answer'>Yes</option><option value=''>No</option></select>\n\nSo as you can see, the p-value for the listened condition is p = .174, and the p-value for the read condition is p = .445. So here we are in an interesting position that often happens. The figures for \"listened\" is a bit unclear, but the figure for \"read\" looks ok and both tests show a non-significant difference from normality. What do we do? Well we combine our knowledge of our data to make a reasoned decision. In this situation the majority of our information is pointing to the data being normal. However, there are known issues with the Shapiro-Wilks test when there are small sample sizes so we must always take results like this with some caution. It is never a good idea to run a small sample such as this and so in reality we might want to design a study that has larger sample groups. All that said, here it would not be unreasonable to take the assumption of normality as being held.\n\n\n::: {.cell layout-align=\"center\" type='info'}\n<div class=\"info\">\n<p>For info though, here are some options if you are convinced your data\nis nor normal.</p>\n<ol style=\"list-style-type: decimal\">\n<li>Transform your data to try and normalise the distribution. We won’t\ncover this but if you’d like to know more, <a\nhref=\"https://www.researchgate.net/profile/Jason_Osborne2/publication/200152356_Notes_on_the_Use_of_Data_Transformations/links/0deec5295f1eb10df8000000.pdf\">this\npage</a> is a good start. Not usually recommended these days but some\nstill use it.</li>\n<li>Use a non-parametric test. The non-parametric equivalent of the\nindependent t-test is the Mann-Whitney and the equivalent of the\npaired-samples t-test is the Wilcoxon signed-ranks test. Though more\nmodern permutation tests are better. Again we won’t cover these here but\nuseful to know if you read them in a paper.</li>\n<li>Do nothing. <a\nhref=\"https://www.rips-irsp.com/articles/10.5334/irsp.82/\">Delacre,\nLakens &amp; Leys, 2017</a> argue that with a large enough sample\n(&gt;30), the Welch test is robust to deviations from assumptions. With\nvery large samples normality is even less of an issue, so design studies\nwith large samples.</li>\n</ol>\n</div>\n:::\n\n\n### Inferential analysis\n\nNow that we have checked our assumptions and our data seems to fit our Welch's t-test we can go ahead and run the test. We are going to conduct t-tests for the Intellect, Hire and Impression ratings separately; each time comparing evaluators' overall ratings for the listened group versus overall ratings for the read group to see if there was a significant difference between the two conditions: i.e. did the evaluators who **listened** to pitches give a significant higher or lower rating than evaluators that **read** pitches.\n\n#### Activity 6: Running the t-test {#ttest-a6}\n\n* First, create separate objects for the intellect, hire, and impression data using `filter()`. We have completed intellect object for you so you should replace the NULLs in the below code to create one for `hire` and `impression`.\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nintellect <- filter(ratings2, Category == \"intellect\")\nhire <- NULL\nimpression <- NULL \n```\n:::\n\n\nAnd we are finally ready to run the t-test. It is funny right, as you may have realised by now, most of the work in analysis involves the set-up and getting the data ready, running the tests is generally just one more function. To conduct the t-test we will use `t.test()` function from `base` which takes the following format called the **formula syntax**:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nt.test(DV_column_name ~ IV_column_name, \n       paired = FALSE,\n       data = my_object)\n```\n:::\n\n\n* `~` is called a tilde. It can be read as 'by' as in \"analyse the DV by the IV\".  \n* The variable on the left of the tilde is the dependent or outcome variable, `DV_column_name`.\n* The variable(s) on the right of the tilde is the independent or predictor variable, `IV_column_name`.  \n* and `paired = FALSE` indicates that we do not want to run a paired-samples test and that our data is from a between-subjects design.\n\nSo let's run our first test:\n\n* In a new code chunk, type and run the below code, and thenview the output by typing `intellect_t` in the console.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nintellect_t <- t.test(Rating ~ condition, \n                      paired = FALSE, \n                      data = intellect,\n                      alternative = \"two.sided\")\n```\n:::\n\n\nSimilar to when we used `cor.test()` for correlations, the output of `t.test()` is a list type object which can make it harder to work with. This time, we are going to show you how to use the function `tidy()` from the `broom` package to convert the output to a tidyverse format.\n\n* Run the below code. You can read it as \"take what is in the object `intellect_t` and try to tidy it into a tibble\".  \n* View the object by clicking on `results_intellect` in the environment.  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nresults_intellect <- intellect_t %>%\n  tidy()\n```\n:::\n\n\nAs you will see, `results_intellect` is now in a nice tibble format that makes it easy to extract individual values. It is worth looking at the values with the below explanations:\n\n* `estimate` is the difference between the two means (alphabetically entered as mean 1 minus mean 2)\n* `estimate1` is the mean of group 1\n* `estimate2` is the mean of group 2  \n* `statistic` is the t-statistic  \n* `p.value` is the p-value  \n* `parameter` is the degrees of freedom  \n* `con.low` and `conf.high` are the confidence interval of the `estimate`\n* `method` is the type of test, Welch's, Student's, paired, or one-sample\n* `alternative` is whether the test was one or two-tailed  \n\nAnd now that we know how to run the test and tidy it, try the below:\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n* Complete the code below in a new code chunk by replacing the NULLs to run the t-tests for the hire and impression ratings, don't tidy them yet.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhire_t <- NULL\nimpression_t <- NULL\n```\n:::\n\n\n* And now tidy the data into the respective objects - `hire_t` into `results_hire`, etc.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nresults_hire <- NULL\nresults_impression <- NULL\n```\n:::\n\n\nBe sure to look at each of your tests and see what the outcome of each was. To make that easier, we are going join all the results of the t-tests together using `bind_rows()` - which we can do because all the tibbles have the same column names after we passed them through `tidy()`. \n\n* Copy and run the below code. First, it specifies all of the individual tibbles you want to join and gives them a label (hire, impression, intellect), and then you specify what the ID column should be named (test).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nresults <- bind_rows(hire = results_hire, \n                     impression = results_impression, \n                     intellect = results_intellect, \n                     .id = \"test\")\n```\n:::\n\n\nWhich produces the below:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n|    test    | estimate | estimate1 | estimate2 | statistic |  p.value  | parameter | conf.low  | conf.high |         method          | alternative |\n|:----------:|:--------:|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|:-----------------------:|:-----------:|\n|    hire    | 1.825397 | 4.714286  | 2.888889  | 2.639949  | 0.0120842 | 36.85591  | 0.4241979 | 3.226596  | Welch Two Sample t-test |  two.sided  |\n| impression | 1.894333 | 5.968333  | 4.074000  | 2.817175  | 0.0080329 | 33.80061  | 0.5275086 | 3.261158  | Welch Two Sample t-test |  two.sided  |\n| intellect  | 1.986722 | 5.635000  | 3.648278  | 3.478555  | 0.0014210 | 33.43481  | 0.8253146 | 3.148130  | Welch Two Sample t-test |  two.sided  |\n:::\n:::\n\n\nAnd looking along the line at the p-values we might have some significant differences. However, we have to remember to consider multiple comparisons.\n\n#### Activity 7: Correcting for multiple comparisons {#ttest-a7}\n\nBecause we have run three t-tests, we are actually increasing our false positive rate due to what is called familywise error - essentially, instead of a false positive rate of .05, we would have a false positive rate of 1-(1-.05)^3 = 0.142625, where the \"3\" in the formula is the number of tests we ran. To correct for this we can apply the multiple comparison correction just like we did with correlations when we ran a lot of correlations. So, we're going to add on a column to our `results` tibble that shows the adjusted p-values using `p.adj()` and `mutate()`. \n\n* Type and run the below code in a new code chunk and have a look at the output.\n  * inside the `p.adjust()`, `p.value` says what column the p-values are in, and `bonferroni` says what adjustment to use.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nresults_adj <- results %>%\n  mutate(p.adjusted = p.adjust(p = p.value, \n                               method = \"bonferroni\"))\n```\n:::\n\n\nLooking at the adjusted p-values, try to answer the following questions:\n\n* Listened is significantly more preferred in the `hire` condition after adjusting for multiple comparisons? <select class='webex-select'><option value='blank'></option><option value='answer'>TRUE</option><option value=''>FALSE</option></select>\n* Listened is significantly more preferred in the `impression` condition after adjusting for multiple comparisons? <select class='webex-select'><option value='blank'></option><option value='answer'>TRUE</option><option value=''>FALSE</option></select>\n* Listened is significantly more preferred in the `intellect` condition after adjusting for multiple comparisons? <select class='webex-select'><option value='blank'></option><option value='answer'>TRUE</option><option value=''>FALSE</option></select>\n\n### Effect Size\n\nAs you can see, even after correcting for multiple comparisons, our effects are still significant and we have maintained our false positive rate. But one more thing we can add is the effect size. Remember that some effects are significant and large, some are significant and medium, and some are significant and small. The effect size tells us the magnitude of the effect size in a way we can compare across studies - it is said to be a standardised - and the common effect size for a t-test is called Cohen's D.\n\n#### Activity 8: Effect size {#ttest-a8}\n\nWhilst Cohen's D is relatively straightforward by hand, here we will use the function `cohens_d()` from the `effectsize` package. The code is similar to the syntax for `t.test()`. \n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n* The code to run the Cohen's D for intellect has been completed below.\n  * The first argument should specify the formula, using the same syntax as `t.test()`, that is `dv ~ iv`.\n  * `pooled_sd` should be `FALSE` if you ran a Welch test where the variances are not assumed to be equal and `TRUE` if you ran a regular Student's t-test.\n* Run and complete the code below by replacing the NULLs to calculate the effect sizes for hire and impression\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nintellect_d <- cohens_d(Rating ~ condition, \n                      pooled_sd = FALSE, \n                      data = intellect)\nhire_d <- NULL\nimpression_d <- NULL\n```\n:::\n\n\n### Interpretation\n\nGreat Work!  But let's take a second to recap on our understanding of the data.\n\n#### Activity 9: Interpreting the results {#ttest-a9}\n\n* Were your results for `hire` significant? Enter the mean estimates and t-test results (means and t-value to 2 decimal places, p-value to 3 decimal places). Use the adjusted p-values:\n\n    + Mean `estimate1` (listened condition) = <input class='webex-solveme nospaces' size='4' data-answer='[\"4.71\"]'/>  \n    \n    + Mean `estimate2` (read condition) = <input class='webex-solveme nospaces' size='4' data-answer='[\"2.89\"]'/>  \n    \n    + t(<input class='webex-solveme nospaces' size='5' data-answer='[\"36.86\"]'/>) = <input class='webex-solveme nospaces' size='4' data-answer='[\"2.64\"]'/>, p = <input class='webex-solveme nospaces' size='5' data-answer='[\"0.036\",\".036\"]'/>  \n    \n\n* Were your results for `impression` significant? Enter the mean estimates and t-test results (means and t-value to 2 decimal places, p-value to 3 decimal places):\n\n    + Mean`estimate1` (listened condition) = <input class='webex-solveme nospaces' size='4' data-answer='[\"5.97\"]'/>  \n    \n    + Mean `estimate2` (read condition) = <input class='webex-solveme nospaces' size='4' data-answer='[\"4.07\"]'/>  \n    \n    + t(<input class='webex-solveme nospaces' size='5' data-answer='[\"33.80\",\"33.8\"]'/>) = <input class='webex-solveme nospaces' size='4' data-answer='[\"2.82\"]'/>, p = <input class='webex-solveme nospaces' size='5' data-answer='[\"0.024\",\".024\"]'/> \n\n* According to Cohen's (1988) guidelines, the effect sizes for all three tests are <select class='webex-select'><option value='blank'></option><option value=''>Small</option><option value=''>Medium</option><option value='answer'>Large</option></select>\n\n### Write-Up\n\nAnd then finally on the between-subjects t-test, we should look at the write up.\n\n#### Activity 10: Write-up {#ttest-a10}\n\nIf you refer back to the original paper on pg 887, you can see, for example, that the authors wrote:\n\n**In particular, the recruiters believed that the job candidates had greater intellect—were more competent, thoughtful, and intelligent—when they listened to pitches (M = 5.63, SD = 1.61) than when they read pitches (M = 3.65, SD = 1.91), t(37) = 3.53, p < .01, 95% CI of the difference = [0.85, 3.13], d = 1.16.**\n\nIf we were to compare our findings, we would have something like the below:\n\n**A bonferroni-corrected Welch t-test found that recruiters rated job candidates as more intellectual when they listened to resumes (M = 5.64, SD = 1.61) than when they read resumes (M = 3.65, SD = 1.91), t(33.43) = 3.48, p = 0.004, 95% CI of the difference = [0.83, 3.15], d = 1.12.**\n\nYou can create this same paragraph, using code, by copying and pasting the below **exactly** into **white space** in your R Markdown document and then knitting the file. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nA bonferroni-corrected Welch t-test found that recruiters rated job candidates as more intellectual when they listened to resumes (M = `r results_intellect$estimate1%>% round(2)`, SD = `r round(group_means$sd[3], 2)`) than when they read resumes (M = `r results_intellect$estimate2%>% round(2)`, SD = `r round(group_means$sd[6], 2)`), t(`r round(results_intellect$parameter, 2)`) = `r round(results_adj$statistic[3],2)`, p = `r results_adj$p.adjusted[3] %>% round(3)`, 95% CI of the difference = [`r round(results_intellect$conf.low, 2)`, `r round(results_intellect$conf.high, 2)`], d = `r round(intellect_d$Cohens_d,2)`. \n```\n:::\n\n\nNote that we haven't replicated the analysis exactly - the authors of this paper conducted Student's t-test whilst we have conducted Welch tests and we've also applied a multiple comparison correction. But you can look at the two examples and see the difference. It would also be worthwhile trying your own write-up of the two remaining conditions before moving on to within-subjects t-tests. \n\n\n## Within-subjects (paired-samples)\n\nFor the final activity we will run a paired-samples t-test for a within-subject design but we will go through this one more quickly and just point out the differences to the above. For this example we will again draw from the [Open Stats Lab](https://sites.trinity.edu/osl/data-sets-and-activities/t-test-activities) and look at data from the data in [Mehr, S. A., Song. L. A., & Spelke, E. S. (2016). For 5-month-old infants, melodies are social. Psychological Science, 27, 486-501.](https://journals.sagepub.com/stoken/default+domain/d5HcBHg85XamSXGdYqYN/full){target = \"_blank\"}. \n\nThe premis of the paper is that parents often sing to their children and, even as infants, children listen to and look at their parents while they are sung to. The authors sought to explore the psychological function that music has for parents and infants, by examining the research question that particular melodies may convey important social information to infants. More specifically, that common knowledge of songs and melodies convey information about social affiliation. The authors argue that melodies are shared within social groups. Whereas children growing up in one culture may be exposed to certain songs as infants (e.g., “Rock-a-bye Baby”), children growing up in other cultures (or even other groups within a culture) may be exposed to different songs. Thus, when a novel person (someone who the infant has never seen before) sings a familiar song, it may signal to the infant that this new person is a member of their social group.\n\nTo test this the researchers recruited 32 infants and their parents to take part in the following experiment. During their first visit to the lab, the parents were taught a new lullaby (one that neither they nor their infants had heard before). The experimenters asked the parents to sing the new lullaby to their child every day for the next 1-2 weeks. Following this 1-2 week exposure period, the parents and their infant returned to the lab to complete the experimental portion of the study. Infants were first shown a screen with side-by-side videos of two unfamiliar people, each of whom were silently smiling and looking at the infant. The researchers recorded the looking behaviour (or gaze) of the infants during this ‘baseline’ phase. Next, one by one, the two unfamiliar people on the screen sang either the lullaby that the parents learned or a different lullaby (that had the same lyrics and rhythm, but a different melody). Finally, the infants saw the same silent video used at baseline, and the researchers again recorded the looking behaviour of the infants during this ‘test’ phase. For more details on the experiment’s methods, please refer to Mehr et al. (2016) Experiment 1. \n\n### The Data\n\n#### Activity 11: Getting the data ready {#ttest-a11}\n\n* First, download <a href=\"Mehr Song and Spelke 2016 Experiment 1.csv\" download>Mehr Song and Spelke 2016 Experiment 1.csv</a> by clicking on the link and putting it into your working directory.\n  * again if easier you can download the data as [a zip file by clicking this link](data/chpt10/PsyTeachR_FQA_Chpt10-data-between.zip).\n* Next, type and run the below code in a new code chunk. The code loads in the data and then does some wrangling to get the data into a working format:\n  * it filters so we just have the first experiment from the paper\n  * selects the id and the preferential looking time of babies at the baseline stage and at the test stage.\n  * finally it renames the two preferential looking time columns to have names that are easier to work with using the `rename()` function.\n  \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngaze <- read_csv(\"Mehr Song and Spelke 2016 Experiment 1.csv\") %>%\n  filter(exp1 == 1) %>%\n  select(id,\n         Baseline_Proportion_Gaze_to_Singer,\n         Test_Proportion_Gaze_to_Singer) %>%\n  rename(baseline = Baseline_Proportion_Gaze_to_Singer,\n         test = Test_Proportion_Gaze_to_Singer)\n```\n:::\n\n\n### Assumptions\n\nSo now that we have our data ready to work with, and be sure to look at it to get an understanding of the data, we want to consider the assumptions of the within-subjects t-test.\n\nThe assumptions for this t-test are a little different (although very similar) to the between-subjects t-tests above. They are\n\n1. The data is continuous, i.e. interval/ratio \n2. All participants should appear in both conditions/groups. \n3. The residuals are normally distributed. \n\nAside from the data being paired rather than independent, i.e. it is the same participants in two conditions, rather than two groups of people in different conditions, the key difference is that for the within-subjects test, the data is actually determined as the difference between the scores in the two conditions for each participant. So for example, say participant one scores 10 in condition 1 and 7 in condition 2, then there data is actually 3, and you do that for all participants. So it isn't looking at what they scored in either condition by itself, but what was the difference between conditions. And it is that data that must be continuous and that the residuals must be normally distributed for.\n\n\n#### Activity 12: Assumptions {#ttest-a12}\n\n* Type and run the below code to first calculate the difference scores (`diff`) and then the residuals (`group_resid`).\n* next it plots the Q-Q plot of the residuals before carrying out a Shapiro-Wilk's test on the residuals\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngaze_residual <- gaze %>%\n  mutate(diff = baseline - test) %>%\n  mutate(group_resid = diff - mean(diff))\n\nqqPlot(gaze_residual$group_resid)\n\nshapiro.test(gaze_residual$group_resid)\n```\n:::\n\n\nAnd if we look at the plot we see:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](10-t-tests_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=100%}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 22 29\n```\n:::\n:::\n\n\nand the Shapiro-Wilk's suggests:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  gaze_residual$group_resid\nW = 0.97818, p-value = 0.7451\n```\n:::\n:::\n\n\nNow as we saw above, with the Q-Q plot we want the data to fall approximately on the diagonal line, and with the Shapiro-Wilks test we are looking for a non-significant finding.  Based on those two tests, we can therefor say that our data meets the assumption of normality and so we can proceed.\n\n### Descriptives\n\nNow we are going to look at some descriptives. It made sense to keep the data in wide-form until this point to make it easy to calculate a column for the difference score, but now we will transform it to tidy data so that we can easily create descriptives and plot the data using `tidyverse` tools.\n\n#### Activity 13: Descriptives and visualisations {#ttest-a13}\n\n* Type and run the below code to gather the data using pivot_longer().\n* Next create a violin-boxplot of the data using your knowledge (and code) from Activity 4 above. \n* Finally, create a descriptives table that contains the n, the mean, and the standard deviation of each condition.\n  * If you prefer, you could actually work on the difference scores instead of the two different conditions. Whilst we analyse the difference, people plot either the difference or the two conditions as descriptives.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngaze_tidy <- gaze %>%\n  pivot_longer(names_to = \"time\", \n               values_to = \"looking\", \n               cols = c(baseline, test))\n```\n:::\n\n\nIf you have done this step correctly, you should see a plot that looks like this:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Preferential Looking time for infants at baseline stage (left) and test stage (right).](10-t-tests_files/figure-html/unnamed-chunk-6-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\nAnd the descriptives:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n|time     |  n| mean_looking| sd_looking|\n|:--------|--:|------------:|----------:|\n|baseline | 32|    0.5210967|  0.1769651|\n|test     | 32|    0.5934912|  0.1786884|\n:::\n:::\n\n\nAgain you could look at the differences and if you know how you could plot the confidence interval of the difference, but it is not essential here.  But looking at what you have done it would be worth spending a few minutes to try and predict the outcome of the t-test if the null hypothesis is that there is no difference in preferential looking time in babies between the baseline and test conditions.\n\n### Inferential Analysis\n\nWhich brings us on to running the t-test and the effect size. The code is almost identical to the independent code with two differences:\n\n1. In `t.test()` you should specify `paired = TRUE` rather than `FALSE`\n2. In `cohens_d()` you should specify `method = paired` rather than `pooled_sd`\n\n#### Activity 14: Paired-samples t-test {#ttest-a14}\n\n* Now have a go at running the within-subjects t-test based on your knowledge. The data you need is in `gaze_tidy()`. Store the output of the t-test as a tibble in the object `gaze_test`\n  * i.e. pipe the output of the t-test into `tidy() in the one line of code.\n* calculate the Cohen's D for the t-test and store it in `gaze_d`\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngaze_test <- NULL\ngaze_d <- NULL\n```\n:::\n\n\nAnd if you have done that correctly, you should see in `gaze_test` something like this:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n|   estimate| statistic|   p.value| parameter|  conf.low|  conf.high|method        |alternative |\n|----------:|---------:|---------:|---------:|---------:|----------:|:-------------|:-----------|\n| -0.0723946|  -2.41643| 0.0217529|        31| -0.133497| -0.0112922|Paired t-test |two.sided   |\n:::\n:::\n\n\n### Write-Up and Interpretation\n\nLooking at the output of the test, it is actually very similar to the between-subjects t-test, with one exception. Rather than providing the means of both conditions, there is a single `estimate`. This is the mean *difference* score between the two conditions and if you had calculated the descriptives on the `diff` we created above you would get the same answer.\n\n* Enter the mean estimates and t-test results (means and t-value to 2 decimal places, p-value to 3 decimal places):\n\n    + Mean `estimate` = <input class='webex-solveme nospaces' size='5' data-answer='[\"-0.07\",\"0.07\"]'/>  \n    \n    + t(<input class='webex-solveme nospaces' size='2' data-answer='[\"31\"]'/>) = <input class='webex-solveme nospaces' size='4' data-answer='[\"2.42\"]'/>, p = <input class='webex-solveme nospaces' size='5' data-answer='[\"0.022\",\".022\"]'/> \n\n#### Activity 15: Write-up {#ttest-a15}\n\nNow have a go at summarising this finding in a sentence using the standard APA formatting. We have hidden our version just below for you to look at when you have had a go.\n\n\n<div class='webex-solution'><button>Show our write-up</button>\n\n\n::: {.cell layout-align=\"center\"}\n\nAt test stage (M = .59, SD = .18), infants showed a significantly longer preferential looking time to the singer of the familiar melody than they had shown the same singer at baseline (M = .52, SD = .18), t(31) = 2.42, p = .022, d = .41.\n\nAlternatively:\n\nAt test stage, infants showed a significantly longer preferential looking time to the singer of the familiar melody than they had shown the same singer at baseline (Mean Difference = 0.07, SD = 0.17), t(31) = 2.42, p = .022, d = .41.\n:::\n\n\n</div>\n\n\n\n## Finished! {#ttest-fin}\n\nThat was a long chapter but hopefully you will see that it really is true that the hardest part is the set-up and the data wrangling. As we've said before, you don't need to memorise lines of code - you just need to remember where to find examples and to understand which bits of them you need to change. Play around with the examples we have given you and see what changing the values does. There is no specific Test Yourself section for this chapter but make sure you check your understanding of the different sections before moving on.\n\n## Activity solutions {#ttest-sols}\n\nBelow you will find the solutions to the above questions. Only look at them after giving the questions a good try and trying to find help on Google or Teams about any issues.\n\n#### Activity 1 {#ttest-a1sol}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(broom)\nlibrary(car)\nlibrary(effectsize)\nlibrary(report)\nlibrary(tidyverse)\nevaluators <- read_csv(\"evaluators.csv\")\n```\n:::\n\n \n\n#### Activity 2 {#ttest-a2sol}\n\nThis was our code:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nevaluators <- evaluators %>%\n  mutate(sex_labels = dplyr::recode(sex, \"1\" = \"male\", \"2\" = \"female\"),\n         sex_labels = as.factor(sex_labels),\n         condition = as.factor(condition))\n```\n:::\n\n\nand you could summarise as below to give an output:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\neval_counts <- group_by(evaluators, sex_labels) %>% count()\n```\n:::\n\n\n#### Activity 6 {#ttest-a6sol}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nintellect <- filter(ratings2, Category == \"intellect\")\nhire <- filter(ratings2, Category == \"hire\")\nimpression <- filter(ratings2, Category == \"impression\")\n```\n:::\n\n\n#### Activity 8 {#ttest-a8sol}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nintellect_d <- cohens_d(Rating ~ condition, \n                      pooled_sd = FALSE, \n                      data = intellect)\nhire_d <- cohens_d(Rating ~ condition, \n                      pooled_sd = FALSE, \n                      data = hire)\nimpression_d <- cohens_d(Rating ~ condition, \n                      pooled_sd = FALSE, \n                      data = impression)\n```\n:::\n\n\n\n#### Activity 13 {#ttest-a13sol}\n\nFor the plot:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(gaze_tidy, aes(x = time, y = looking)) +\n  geom_violin(trim = FALSE) +\n  geom_boxplot(aes(fill = time), width = .2, show.legend = FALSE) + \n  stat_summary(geom = \"pointrange\", fun.data = \"mean_cl_normal\") +\n  labs(x = \"Experimental Stage\", \n       y = \"Preferential Looking Time (Proportion)\")\n```\n:::\n\n\nFor the descriptives:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndesc <- gaze_tidy %>% \n  group_by(time) %>% \n  summarise(n = n(), \n            mean_looking = mean(looking), \n            sd_looking = sd(looking))\n```\n:::\n\n\n#### Activity 14 {#ttest-a14sol}\n\nFor the t-test:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngaze_test <- t.test(looking ~ time, \n                    paired = TRUE, \n                    data = gaze_tidy) %>% \n  tidy()\n```\n:::\n\n\nFor the Cohen's D:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngaze_d <- cohens_d(looking ~ time, \n                   method = \"paired\", \n                   data = gaze_tidy)\n```\n:::\n\n\n## Words from this Chapter\n\nBelow you will find a list of words that were used in this chapter that might be new to you in case it helps to have somewhere to refer back to what they mean. The links in this table take you to the entry for the words in the [PsyTeachR Glossary](https://psyteachr.github.io/glossary/){target=\"_blank\"}. Note that the Glossary is written by numerous members of the team and as such may use slightly different terminology from that shown in the chapter.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:left;\"> definition </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> between subjects </td>\n   <td style=\"text-align:left;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> categorical </td>\n   <td style=\"text-align:left;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> mixed design </td>\n   <td style=\"text-align:left;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> numeric </td>\n   <td style=\"text-align:left;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> One-sample </td>\n   <td style=\"text-align:left;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> within subjects </td>\n   <td style=\"text-align:left;\">  </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nThat is end of this chapter. Be sure to look again at anything you were unsure about and make some notes to help develop your own knowledge and skills. It would be good to write yourself some questions about what you are unsure of and see if you can answer them later or speak to someone about them. Good work today!\n",
    "supporting": [
      "10-t-tests_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}