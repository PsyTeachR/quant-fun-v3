{
  "hash": "f8da67632e96a8348f3b077d88014a5b",
  "result": {
    "markdown": "# Multiple regression\n\nThere is currently much debate (and hype) surrounding smartphones and their effects on well-being, especially with regard to children and teenagers.  We'll be looking at data from this recent study of English adolescents:\n\n> Przybylski, A. & Weinstein, N. (2017). A Large-Scale Test of the Goldilocks Hypothesis. *Psychological Science*, *28*, 204--215.\n\nThis was a large-scale study that found support for the \"Goldilocks\" hypothesis among adolescents: that there is a \"just right\" amount of screen time, such that any amount more or less than this amount is associated with lower well-being.  This was a huge survey study: the data contain responses from over 120,000 participants! \n\nFortunately, the authors made the data from this study openly available, which allows us to dig deeper into their results. In this exercise, we will look at whether the relationship between screen time and well-being is modulated by participants' (self-reported) gender.\n\nThe dependent measure used in the study was the [Warwick-Edinburgh Mental Well-Being Scale (WEMWBS)](https://warwick.ac.uk/fac/med/research/platform/wemwbs/). This is a 14-item scale with 5 response categories, summed together to form a single score ranging from 14-70.\n\nAt [Przybylski & Weinstein's page for this study on the Open Science Framework](https://osf.io/82ybd/), you can find the [participant survey](https://osf.io/82ybd/) which asks a large number of additional questions (see page 14 for the WEMWBS questions and pages 4-5 for the questions about screen time). Within the same page you can also find the [raw data](https://osf.io/82ybd/); however, for the purpose of this exercise, you will be using local pre-processed copies of the data which we will provide.\n\nPrzybylski and Weinstein looked at multiple measures of screen time, but we will be focusing on smartphone use.  They found that decrements in well-being started to appear when respondents reported more than one hour of weekly smartphone use.  Our question: Does the negative association between hours of use and well-being (beyond the one-hour point) differ for boys and girls?\n\nNote that in this analysis, we have:\n\n- a continuous$^*$ DV, well-being;\n\n- a continuous$^*$ predictor, screen time;\n\n- a categorical predictor, gender.\n\n$^*$these variables are only quasi-continuous, inasmuch as only discrete values are possible. However, there are a sufficient number of discrete categories that we can treat them as effectively continuous.\n\nWe want to estimate two slopes relating screen time to well-being, one for girls and one for boys, and then statistically compare these slopes. So this problem seems simultaneously like a situation where you would run a regression (to estimate the slopes) but also one where you would need a t-test (to compare two groups).\n\nBut the expressive power of regression allows us to do this all within a single model. As the [Bishop blog showed](http://deevybee.blogspot.com/2017/11/anova-t-tests-and-regression-different.html), *an independent groups t-test is just a special case of ordinary regression with a single categorical predictor; ANOVA is just a special case of regression where all predictors are categorical.*  So although we can express any ANOVA design using regression, the converse is not true: we cannot express every regression design in ANOVA. Regression allows us to have any combination of continuous and categorical predictors in the model. The only inconvenience with running ANOVA models as regression models is that you have to take care in how you numerically code the categorical predictors.\n\n## Activity 1: Set-up {#mulregression-a1}\n\n* Open R Studio and set the working directory to your chapter folder. Ensure the environment is clear.    \n* Open a new R Markdown document and save it in your working directory. Call the file \"Multiple Regression\".    \n* Download <a href=\"wellbeing.csv\" download>wellbeing.csv</a>, <a href=\"participant_info.csv\" download>participant_info.csv</a> and <a href=\"screen_time.csv\" download>screen_time.csv</a> and save them in your Chapter folder. Make sure that you do not change the file names at all.    \n* If you're on the server, avoid a number of issues by restarting the session - click `Session` - `Restart R` \n* Delete the default R Markdown welcome text and insert a new code chunk that loads `pwr`, `see`, `performance`, `report`, and `tidyverse` using the `library()` function.\n* Load the CSV datasets into variables called `pinfo`, `wellbeing` and `screen` using `read_csv()`.\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n## Activity 2: Look at the data {#mulregression-a2}\n\nTake a look at the resulting tibbles `pinfo`, `wellbeing`, and `screen`.  The `wellbeing` tibble has information from the WEMWBS questionnaire; `screen` has information about screen time use on weekends (variables ending with `we`) and weekdays (variables ending with `wk`) for four types of activities: using a computer (variables starting with `Comph`; Q10 on the survey), playing video games (variables starting with `Comp`; Q9 on the survey), using a smartphone (variables starting with `Smart`; Q11 on the survey) and watching TV (variables starting with `Watch`; Q8 on the survey).  If you want more information about these variables, look at the items 8-11 on pages 4-5 of the the [PDF version of the survey on the OSF website](https://osf.io/82ybd/).\n\n* The variable corresponding to *gender* is located in the table named <select class='webex-select'><option value='blank'></option><option value='answer'>pinfo</option><option value=''>wellbeing</option><option value=''>screen</option></select> and this variable is called <input class='webex-solveme nospaces' size='6' data-answer='[\"male\"]'/>.\n\n* The WEMWBS data is in <select class='webex-select'><option value='blank'></option><option value=''>long</option><option value='answer'>wide</option></select> format, and contains observations from <input class='webex-solveme nospaces' size='10' data-answer='[\"102580\",\"102,580\"]'/> participants on <input class='webex-solveme nospaces' size='2' data-answer='[\"15\"]'/> items.\n\n* Individual participants in this dataset are identified by the variable named <input class='webex-solveme nospaces' size='9' data-answer='[\"Serial\"]'/> [be sure to type the name *exactly*, including capitalization].  This variable will allow us to link information across the three tables.\n\n* Run `summary()` on the three data-sets. Are there any missing data points? <select class='webex-select'><option value='blank'></option><option value=''>Yes</option><option value='answer'>No</option></select>\n\n\n## Activity 3: Compute the well-being score for each respondent {#mulregression-a3}\n\nThe WEMWBS well-being score is simply the *sum* of all the items. \n\n* Write the code to create a new table called `wemwbs`, with two variables: `Serial` (the participant ID), and `tot_wellbeing`, the total WEMWBS score.\n\n\n<div class='webex-solution'><button>Hint</button>\n\n- \"pivot\" the table from wide to long\n\n</div>\n\n\n\n<div class='webex-solution'><button>Another Hint</button>\n\n- `group_by()`; `summarise(tot_wellbeing = ...)`\n\n</div>\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n**Sanity check:** Verify for yourself that the scores all fall in the 14-70 range.  Przybylski and Weinstein reported a mean of 47.52 with a standard deviation of 9.55. Can you reproduce these values?\n\n\n<div class='webex-solution'><button>Hint</button>\n\n- `summarise()`, `min()`, `max()`\n\n</div>\n\n<br>\n\n* Now visualise the distribution of `tot_wellbeing` in a histogram using ggplot2.  \n\n\n<div class='webex-solution'><button>Hint</button>\n\n- `geom_histogram()`\n\n</div>\n\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(wemwbs, aes(tot_wellbeing)) + geom_histogram() \n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](17-multiple-regression_files/figure-html/wemwbs_histogram-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n</div>\n\n\nThe distribution of well-being scores is <select class='webex-select'><option value='blank'></option><option value=''>symmetric</option><option value='answer'>negatively skewed</option><option value=''>positively skewed</option></select>.\n\n## Activity 4: Visualise the relationship {#mulregression-a4}\n\nLet's take a quick look at the relationship between screen time (for the four different technologies) and measures of well-being.  Here is code to do this. \n\n* Run the below code and try and explain in words what each line of code is doing (remember, pronounce `%>%` as \"and then\"). You may find it easier to look at each of the tables that are produced.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nscreen_long <- screen %>%\n  pivot_longer(names_to = \"var\", values_to = \"hours\", -Serial) %>%\n  separate(var, c(\"variable\", \"day\"), \"_\")\n\nscreen2 <- screen_long %>%\n  mutate(variable = dplyr::recode(variable,\n               \"Watch\" = \"Watching TV\",\n               \"Comp\" = \"Playing Video Games\",\n               \"Comph\" = \"Using Computers\",\n               \"Smart\" = \"Using Smartphone\"),\n     day = dplyr::recode(day,\n              \"wk\" = \"Weekday\",\n              \"we\" = \"Weekend\"))\n\ndat_means <- inner_join(wemwbs, screen2, \"Serial\") %>%\n  group_by(variable, day, hours) %>%\n  summarise(mean_wellbeing = mean(tot_wellbeing))\n\nggplot(dat_means, aes(hours, mean_wellbeing, linetype = day)) +\n  geom_line() +\n  geom_point() +\n  facet_wrap(~variable, nrow = 2)\n```\n\n::: {.cell-output-display}\n![Relationship between wellbeing and screentime usage by technology and weekday](17-multiple-regression_files/figure-html/combined-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\nThe graph makes it evident that smartphone use of more than 1 hour per day is associated with increasingly negative well-being.  Note that we have combined the tables using an `inner_join()`, such that we only include data for which we have observations across the `wemwbs` and `screen2` tables.\n\nIn the next step, we are going to focus in on the smartphone/well-being relationship.\n\n## Activity 5: Smartphone and well-being for boys and girls {#mulregression-a5}\n\nFor this analysis, we are going to collapse weekday and weekend use for smartphones.\n\n* Create a new table, `smarttot`, that has the that has mean number of hours per day of smartphone use for each participant, averaged over weekends/weekdays. \n* You will need to filter the dataset to only include smartphone use and not other technologies. \n* You will also need to group the results by the participant ID (i.e., `serial`). \n* The final data-set should have two variables: `Serial` (the participant) and `tothours`.\n* You will need to use the data-set `screen2` to do this.\n\n\n<div class='webex-solution'><button>Hint</button>\n\n- `filter()` then `group_by()` then `summarise()`\n\n</div>\n\n\n* Next, create a new tibble called `smart_wb` that only includes (filters) participants from `smarttot` who used a smartphone for more than one hour per day each week, and then combine (join) this table with the information in `wemwbs` and `pinfo`.**\n\n\n<div class='webex-solution'><button>Hint</button>\n\n- `filter()` then `inner_join()` then another `inner_join()`\n\n</div>\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n## Activity 6: Mean-centering variables {#mulregression-a6}\n\nAs discussed in the lecture, When you have continuous variables in a regression, it is often sensible to transform them by *mean centering*.  You mean center a predictor `X` simply by subtracting the mean (`X_centered = X - mean(X)`). This has two useful consequences:\n\n- the model intercept reflects the prediction for $Y$ at the mean value of the predictor variable, rather than at the zero value of the unscaled variable;\n\n- if there are interactions in the model, any lower-order effects can be given the same interpretation as they receive in ANOVA (main effects, rather than simple effects).\n\nFor categorical predictors with two levels, these become coded as -.5 and .5 (because the mean of these two values is 0).\n\n* Use `mutate` to add two new variables to `smart_wb`: `tothours_c`, calculated as a mean-centered version of the `tothours` predictor; and `male_c`, recoded as -.5 for female and .5 for male.\n* To create `male_c` you will need to use `if_else(male == 1, .5, -.5)` You can read this code as \"if the variable `male` equals 1, recode it as .5, if not, recode it as -.5\".\n* Finally, recode `male` and `male_c` as factors, so that R knows not to treat them as a real numbers.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n## Activity 7: Visualise the relationship {#mulregression-a7}\n\n* Reverse-engineer the below plot. Calculate mean well-being scores for each combination of `male` and `tothours`, and then create a scatterplot plot that includes separate regression lines for each gender.\n* You may find it useful to refer to the Visualisation chapter.\n\n\n<div class='webex-solution'><button>Hint</button>\n\n- `group_by()` both variables then `summarise()`\n- `colour = variable_you_want_different_colours_for`\n\n</div>\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Relationship between mean wellbeing and smartphone use by gender](17-multiple-regression_files/figure-html/plots-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\nWrite an interpretation of the above plot in plain English.\n\n\n<div class='webex-solution'><button>Possible solution</button>\n\nGirls show lower overall well-being compared to boys.  In addition, the slope for girls appears more negative than that for boys; the one for boys appears relatively flat.  This suggests that the negative association between well-being and smartphone use is stronger for girls.\n\n</div>\n\n\n## Activity 8: Running the regression {#mulregression-a8}\n\nNow we're going to see if there is statistical support for our above interpretation of the graph.\n\nFor the data in `smart_wb`, use the `lm()` function to calculate the multiple regression model:\n\n$Y_i = \\beta_0 + \\beta_1 X_{1i}  + \\beta_2 X_{2i}  + \\beta_3 X_{3i} + e_i$\n\nwhere\n\n- $Y_i$ is the well-being score for participant $i$;\n- $X_{1i}$ is the mean-centered smartphone use variable for participant $i$;\n- $X_{2i}$ is gender (-.5 = female, .5 = male);\n- $X_{3i}$ is the interaction between smartphone use and gender ($= X_{1i} \\times X_{2i}$)\n\nThen use `summary()` to view the results and store this in an object called `mod_summary()`.\n\n\n<div class='webex-solution'><button>Hint</button>\n\n- R formulas look like this: `y ~ a + b + a:b` where `a:b` means interaction\n\n</div>\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n* The interaction between smartphone use and gender is shown by the variable <select class='webex-select'><option value='blank'></option><option value=''>thours_c</option><option value=''>male_c</option><option value='answer'>thours_c:male_c</option></select>, and this interaction was <select class='webex-select'><option value='blank'></option><option value='answer'>significant</option><option value=''>nonsignificant</option></select> at the $\\alpha = .05$ level.\n\n* To 2 decimal places, what proportion of the variance in well-being scores does the overall model explain? <input class='webex-solveme nospaces' size='4' data-answer='[\"9.38\"]'/>\n\n* The p-value for the overall model fit is `< 2.2e-16`. Is this significant? <select class='webex-select'><option value='blank'></option><option value='answer'>Yes</option><option value=''>No</option></select>\n\n* What is the most reasonable interpretation of these results? <select class='webex-select'><option value='blank'></option><option value=''>smartphone use harms girls more than boys</option><option value=''>smartphone use harms boys more than girls</option><option value=''>there is no evidence for gender differences in the relationship between smartphone use and well-being</option><option value='answer'>smartphone use was more negatively associated with wellbeing for girls than for boys</option></select>\n\n## Activity 9: Assumption checking {#mulregression-a9}\n\nNow it's time to test those pesky assumptions. The assumptions for multiple regression are the same as simple regression but there is one additional assumption, that of multicollinearity, the idea that predictor variables should not be too highly correlated.\n\n1. The outcome/DV is a interval/ratio level data \n2. The predictor variable is interval/ratio or categorical (with two levels)\n3. All values of the outcome variable are independent (i.e., each score should come from a different participant)\n4. The predictors have non-zero variance\n5. The relationship between outcome and predictor is linear\n6. The residuals should be normally distributed\n7. There should be homoscedasticity (homogeneity of variance, but for the residuals)\n8. Multicollinearity: predictor variables should not be too highly correlated\n\nFrom the work we've done so far we know that assumptions 1 - 4 are met and we can use the functions from the `performance` package again to check the rest, like we did with the simple linear regression chapter.\n\nOne difference from when we used `check_model()` previously is that rather than just letting it run all the tests it wants, we're going to specify which tests, to stop it throwing an error. A word of warning - these assumptions tests will take longer than usual to run, because it's such a big dataset. The first line of code will run the assumption tests and save it to an object, calling the object name will then display the plots.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nassumptions <- check_model(mod, check = c(\"vif\", \"qq\", \"normality\", \"linearity\", \"homogeneity\"))\n\nassumptions\n```\n\n::: {.cell-output-display}\n![Assumption plots](17-multiple-regression_files/figure-html/unnamed-chunk-1-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\nFor assumption 5, linearity, we already know from looking at the scatterplot that the relationship is linear, but the residual plot also confirms this.\n\nFor assumption 6, normality of residuals, again the residuals look good in both plots and this provides an excellent example of why it's often better to visualise than rely on statistics because if we use `check_normality()` which calls the Shapiro-Wilk test:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncheck_normality(mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWarning: Non-normality of residuals detected (p < .001).\n```\n:::\n:::\n\n\nIt tells us that the residuals are not normal, despite the fact that the plots look almost perfect. And that's because with large sample sizes, any deviation from perfect normality can be flagged as non-normal.\n\nFor assumption 7, homoscedasticity, the plot is missing the reference line - fun fact, this took us several days of our lives and asking for help on Twitter to figure out. The reason the line isn't there is because the dataset is so large that is creates a memory issue so we need to create the plot ourselves using code the developers of the package `see` provided to us on Twitter. The default code would try to draw confidence intervals around the line which is what causes the memory issue, this code removes that with `se = FALSE`.\n\nPlease note that with most datasets you wouldn't have to do this extra step, but it's a good example that when it comes to programming, it doesn't matter how long you've been doing it, there will always be a problem you haven't come across and that asking for help is part of the process.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(assumptions$HOMOGENEITY, aes(x, y)) +\n    geom_point2() +\n    stat_smooth(\n      method = \"loess\",\n      se = FALSE,\n      formula = y ~ x,\n    ) +\n    labs(\n      title = \"Homogeneity of Variance\",\n      subtitle = \"Reference line should be flat and horizontal\",\n      y = expression(sqrt(\"|Std. residuals|\")),\n      x = \"Fitted values\"\n    ) \n```\n\n::: {.cell-output-display}\n![Adjusted homogeneity plot that will produce reference line](17-multiple-regression_files/figure-html/unnamed-chunk-3-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\nAgain like normality, the plot isn't perfect but it is pretty good and another example of why visualisation is better than running statistical tests as we see the same significant result if we run:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncheck_homogeneity(mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWarning: Variances differ between groups (Bartlett Test, p = 0.000).\n```\n:::\n:::\n\n\n\nFor assumption 8, linearity, again the plot looks fine, and we could also have used the grouped scatterplots above to look at this. \n\nFinally, for assumption 9, multicollinearity, the plot also indicates no issues but we can also test this statistically using `check_collinearity()`.\n\nEssentially, this function estimates how much the variance of a coefficient is “inflated” because of linear dependence with other predictors, i.e., that a predictor isn't actually adding any unique variance to the model, it's just really strongly related to other predictors. [You can read more about this here](https://statisticalhorizons.com/multicollinearity). Thankfully, VIF is not affected by large samples like the other tests.\n\nThere are various rules of thumb, but most converge on a VIF of above 2 - 2.5 for any one predictor being problematic.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncheck_collinearity(mod)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|Term            |      VIF| VIF_CI_low| VIF_CI_high| SE_factor| Tolerance| Tolerance_CI_low| Tolerance_CI_high|\n|:---------------|--------:|----------:|-----------:|---------:|---------:|----------------:|-----------------:|\n|thours_c        | 1.721968|   1.704219|    1.740165|  1.312238| 0.5807308|        0.5746582|         0.5867789|\n|male_c          | 1.035552|   1.028488|    1.044369|  1.017621| 0.9656682|        0.9575159|         0.9723014|\n|thours_c:male_c | 1.716349|   1.698683|    1.734463|  1.310095| 0.5826319|        0.5765474|         0.5886915|\n\n</div>\n:::\n:::\n\n\n## Activity 10: Power and effect size {#mulregression-a10}\n\nFinally, we'll calculate power and effect size as usual.\n\n* Using the code from Power and Effect Size calculate the minimum effect size we could reliably observe given our sample size and design but for 99% power. Report this to 2 decimal places <input class='webex-solveme nospaces' size='0.00' data-answer='[\".00\"]'/>\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n* What is the observed effect size for the study to 2 decimal places? <input class='webex-solveme nospaces' size='0.10' data-answer='[\".10\"]'/>  \n* Is the study sufficiently powered? <select class='webex-select'><option value='blank'></option><option value='answer'>Yes</option><option value=''>No</option></select>\n\n## Activity 11: Write-up {#mulregression-a11}\n\nSame as the simple regression, we can use inline coding or the `report()` function to help with the write-up. First, copy and paste the below code into **white-space** and then knit the document. Note that the p-values are entered manually because of the APA `p < .001` formatting.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nAll continuous predictors were mean-centered and deviation coding was used for categorical predictors. The results of the regression indicated that the model significantly predicted course engagement (F(`r mod_summary$fstatistic[2]`, `r mod_summary$fstatistic[3] %>% round(2)`) = `r mod_summary$fstatistic[1] %>% round(2)`, p < .001, Adjusted R2 = `r mod_summary$adj.r.squared %>% round(2)`, f^2^ = .63), accounting for `r (mod_summary$adj.r.squared %>% round(2))*100`% of the variance. Total screen time was a significant negative predictor of wellbeing scores (β = `r mod$coefficients[2] %>% round(2)`, p < .001, as was gender (β = `r mod$coefficients[3] %>% round(2)`, p < .001, with girls having lower wellbeing scores than boys. Importantly, there was a significant interaction between screentime and gender (β = `r mod$coefficients[4] %>% round(2)`, p < .001), smartphone use was more negatively associated with wellbeing for girls than for boys. \n```\n:::\n\n\n> All continuous predictors were mean-centered and deviation coding was used for categorical predictors. The results of the regression indicated that the model significantly predicted course engagement (F(3, 7.1029\\times 10^{4}) = 2450.89, p < .001, Adjusted R2 = 0.09, f2 = .63), accounting for 9% of the variance. Total screen time was a significant negative predictor of well-being scores (β = -0.77, p < .001, as was gender (β = 5.14, p < .001, with girls having lower well-being scores than boys. Importantly, there was a significant interaction between screen time and gender (β = 0.45, p < .001), smartphone use was more negatively associated with well-being for girls than for boys.\n\nNow, we can use `report()` to produce an automated summary. Again, it would need some editing but may be useful to aid interpretation and reporting.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nreport(mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWe fitted a linear model (estimated using OLS) to predict tot_wellbeing with\nthours_c and male_c (formula: tot_wellbeing ~ thours_c * male_c). The model\nexplains a statistically significant and weak proportion of variance (R2 =\n0.09, F(3, 71029) = 2450.89, p < .001, adj. R2 = 0.09). The model's intercept,\ncorresponding to thours_c = 0 and male_c = -0.5, is at 44.87 (95% CI [44.78,\n44.96], t(71029) = 1001.87, p < .001). Within this model:\n\n  - The effect of thours c is statistically significant and negative (beta =\n-0.77, 95% CI [-0.82, -0.73], t(71029) = -32.96, p < .001; Std. beta = -0.15,\n95% CI [-0.16, -0.15])\n  - The effect of male c [0.5] is statistically significant and positive (beta =\n5.14, 95% CI [5.00, 5.28], t(71029) = 72.25, p < .001; Std. beta = 0.54, 95% CI\n[0.52, 0.55])\n  - The effect of thours c × male c [0.5] is statistically significant and\npositive (beta = 0.45, 95% CI [0.38, 0.52], t(71029) = 12.24, p < .001; Std.\nbeta = 0.09, 95% CI [0.08, 0.11])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n```\n:::\n:::\n\n\n## Finished! {#mulregression-fin}\n\nAnd you're done! Not just with this week but with the R component of RM2!  The progress that you have made is truly astonishing. Even if you struggled with R and haven't quite understood every single line of code we've shown, what you're capable of with data wrangling and visualisation alone makes you some of the most highly competitive psychology graduates in the world. \n\nRegardless of whether you continue with quantitative methods and using R, remember the more important critical skills that you have learned as part of this process. The next time you see a dataset or you see data being talked about in the news, think about all work that was put into getting the data into the final format. More importantly, think about all the decisions that the researcher needed to make along the way and how that might have affected the outcome. \n\n![](https://media.giphy.com/media/ujGfBmVppmgEg/giphy.gif) \n\n\n## Activity solutions {#mulregression-sols}\n\n### Activity 3 {#mulregression-a3sol}\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nwemwbs <- wellbeing %>%\n  pivot_longer(names_to = \"var\", values_to = \"score\", -Serial) %>%\n  group_by(Serial) %>%\n  summarise(tot_wellbeing = sum(score))\n\n# sanity check values\n\nwemwbs %>% summarise(mean = mean(tot_wellbeing),\n                     sd = sd(tot_wellbeing),\n                     min = min(tot_wellbeing), \n                     max = max(tot_wellbeing))\n```\n:::\n\n\n</div>\n\n\n### Activity 5 {#mulregression-a5sol}\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsmarttot <- screen2 %>%\n  filter(variable == \"Using Smartphone\") %>%\n  group_by(Serial) %>%\n  summarise(tothours = mean(hours))\n\nsmart_wb <- smarttot %>%\n  filter(tothours > 1) %>%\n  inner_join(wemwbs, \"Serial\") %>%\n  inner_join(pinfo, \"Serial\") \n```\n:::\n\n\n</div>\n\n\n### Activity 6 {#mulregression-a6sol}\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsmart_wb <- smarttot %>%\n  filter(tothours > 1) %>%\n  inner_join(wemwbs, \"Serial\") %>%\n  inner_join(pinfo, \"Serial\") %>%\n  mutate(thours_c = tothours - mean(tothours),\n         male_c = ifelse(male == 1, .5, -.5),\n         male_c = as.factor(male_c),\n         male = as.factor(male))\n```\n:::\n\n\n</div>\n\n\n### Activity 7 {#mulregression-a7sol}\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsmart_wb_gen <- smart_wb %>%\n  group_by(tothours, male) %>%\n  summarise(mean_wellbeing = mean(tot_wellbeing))\n\nggplot(smart_wb_gen, aes(tothours, mean_wellbeing, color = male)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  scale_color_discrete(name = \"Gender\", labels = c(\"Female\", \"Male\"))+\n  scale_x_continuous(name = \"Total hours smartphone use\") +\n  scale_y_continuous(name = \"Mean well-being score\")\n```\n:::\n\n\n</div>\n\n\n### Activity 8 {#mulregression-a8sol}\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmod <- lm(tot_wellbeing ~ thours_c * male_c, smart_wb)\n# alternatively: \n# mod <- lm(tot_wellbeing ~ thours_c + male_c + thours_c:male_c, smart_wb)\n\nmod_summary <- summary(mod)\n```\n:::\n\n\n</div>\n\n\n### Activity 9 {#mulregression-a9sol}\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nqqPlot(mod$residuals)\n```\n:::\n\n\n</div>\n\n\n### Activity 10 {#mulregression-a10sol}\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npwr.f2.test(u = 3, v = 71029, f2 = NULL, sig.level = .05, power = .99)\nf2 <- mod_summary$adj.r.squared/(1 - mod_summary$adj.r.squared)\n```\n:::\n\n\n</div>\n\n",
    "supporting": [
      "17-multiple-regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}